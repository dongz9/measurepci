\section{PCIe and InfiniBand}
\label{sec:pcie}

This section discusses relevant details of PCI Express and InfiniBand, and
discusses what PCIe events are measured by Intel's PCIe counters on different
CPU generations.

\subsection{PCI Express}
PCIe is a serial point-to-point interconnect which is commonly used to connect
peripheral devices to a CPU. The bandwidth of a PCIe link is determined by two
factors: the link width and the PCIe generation. The link width speficies the
number of parallel lanes in the interconnect---the bandwidth of the link is
simply the per-lane bandwidth multiplied by the number of lanes. The PCIe
generation can be any number between 1 and 3, with PCIe 3.0 being the newest
generation at the time of writing. The transfer rate of a PCIe lane has
increased with each PCIe generation; Table~\ref{table:pcie} shows the transfer
rate of different PCIe generations and the bandwidth of a single lane of this
generation. As diffenent PCIe generations use different physical layer encodings
(8b/10b for PCIe 1.0 and 2.0, 128b/130b for PCIe 3.0), the relation between the
transfer rate and useful bandwidth of a PCIe lane depends on the generation. 

\begin{table}
\begin{center}
    \begin{tabular}{p{2.5cm} p{1.5cm} p{2.5cm}}
	\textbf{Generation} & \textbf{Bitrate} & \textbf{Lane bandwidth}\\
    \hline
	PCIe 1.0 & 2.5 GT/s & 250 MB/s \\
	PCIe 2.0 & 5 GT/s & 500 MB/s \\
	PCIe 3.0 & 8 GT/s & 984.6 MB/s \\
    \end{tabular}
\caption{Transfer rate and per-lane bandwidth of different PCIe generations}
\label{table:pcie}
\end{center}
\end{table}

\subsubsection{PCIe protocol}
PCIe is a layered protocol and consists of a physical layer, a link layer,
and a transport layer. The link layer uses credit-based flow control and
acknowledgments to provide reliable delivery to the transport layer.
Figure~\ref{fig:pcie-tlp} shows the structure of a PCIe 3.0 transaction layer
packet (PCIe TLP). In addition to the data payload, the packet includes physical
layer framing, a link layer sequence number (for reliability), a transport layer
header, and checksum at both the transport and link layer.

For the purposes of this paper, we assume that there are two types of PCIe
operations: memory read and memory write. The type of operation and the memory
address (in the peer's virtual memory) to read or write from is specified
in the transaction layer header. Therefore, the transaction layer
header can be either 12 or 16 bytes depending on whether 32-bit or 64-bit
addressing is used.  As almost all modern servers use 64-bit addressing, we
assume that the size of the transaction layer header is 16 bytes.

Note that the exact method of generating PCIe read and write transactions is
different for CPUs and NICs. The two common methods for generating these
transactions are Mapped Memory I/O (MMIO) and Direct Memory Access (DMA). MMIO
is used by the CPU to read or write the NIC's memory, whereas the NIC uses
DMA to read or write the CPU's memory. The efficiency of these two methods in
terms of CPU utilization (CPU utilization is reduced if a CPU-initiated MMIO
write can be replaced by a NIC-initiated DMA read) and PCIe bandwidth (MMIO
operations are restricted to a cacheline granularity) is different, which leads
to interesting optimizations and tradeoffs~\ref{sec:measurements}.

From Figure~\ref{fig:pcie-tlp}, it is clear that the minimum TLP overhead of
a PCIe packet is 30 bytes. This overhead is comparable to the common size of
data items used in networked services such as memcached~\cite{Nishtala:nsdi2013}
and RPCs~\cite{Flajslik:usenix2013}. Understanding and mitigating this overhead
can help in to designing RDMA-based communication protocols and networked
systems, as we will show in Section~\ref{sec:measurements}.

\begin{figure}
	\centering
	\includegraphics[width=.48\textwidth]{figures/pcie-tlp.png}
	\caption{\textbf{Structure of a PCIe TLP. The diagram was copied from~\cite{www-xilinx-pcie}}}
	\label{fig:pcie-tlp}
\end{figure}

\subsection{InfiniBand}
InfiniBand is standard for high-speed communication, commonly deployed between
two servers in a datacenters. InfiniBand focuses on low-latency and
high-bandwidth communication. The InfiniBand adapters used in this work have 56
Gbps of peak theoretical bandwidth per port, with $\sim$ 2 microseconds of
round-trip latency. All InfiniBand-compliant adapters must also provide RDMA.

\subsubsection{InfiniBand protocol}
Similar to PCIe, InfiniBand is a layered protocol. The header format of an
InfiniBand packet depends on several factors including the transport (connected
or datagram) and the type of RDMA operation (RDMA write or read or atomic). We
omit repeating all the header fields in this paper because they are irrelevant
from the PCIe point of view.

\subsubsection{CPU-NIC interaction for RDMA}
InfiniBand communication uses queue pairs at the end hosts. To initiate an
InfiniBand operation, a user-level NIC driver at the requester host creates
a Work Queue Element (WQE) in host memory\footnote{We call the memory (cache
or DRAM) connected to a host CPU ``host memory'' and the memory (SRAM) of a NIC
``device memory''.}. This WQE needs to be transferred from host memory to device
memory over the PCIe interconnect. There are 3 possible methods:

\begin{enumerate}
\item \textbf{Doorbell method}: In this method, 
\end{enumerate}
