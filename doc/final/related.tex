\section{Related Work}
\label{sec:related}

\subsection{Interaction between NICs and CPUs}

Previous literatures attmpted to understand the interaction between
NICs and CPUs and its impact on the network latency and
throughput. For example, Larsen et. al~\cite{larsen:2011,larsen:2014}
and Flajslik et. al~\cite{Flajslik:usenix2013} reported the transmit/receive
flow of typical Ethernet NICs on modern Intel platforms. Larsen
et. al~\cite{larsen:2011,larsen:2014} further used a PCIe protocol
analyzer to quantitively breakdown the end-to-end network latency
between two systems. Their results demonstrated a substantial portion
of latency is introduced by DMA reads performed at TX NIC, which pull
the data from the system memory to the TX buffer on the TX
NIC. Moreover, they showed that when transmitting small packets (64B
in their experiment), an aggregated PCIe bandwidth of up to 43\% were
spent on non-payload traffic. 

Our work uses CPU performance counters, instead of a separate PCIe
protocol analyzer, to infer the interactions with InfiniBand NICs and
CPUs. We focus on the behavior of InfiniBand when
transmitting/receiving a batch of packets instead of a single packet.

\subsection{Better NIC Design}

Several new solutions have been proposed to overcome the shortcomings
of current Ethernet NIC design, especially when transmitting/receiving
small packets. For example, Larsen
et. al~\cite{larsen:2011,larsen:2014} proposed a new DMA mechanism
called integrated DMA (iDMA), which essentially moves the DMA engine
from the PCIe devices to CPU to eliminate PICe transactions involved
with TX/RX doorbells and packet descriptors. Similarly, Flajslik
et. al~\cite{Flajslik:usenix2013} suggested that the small packet (64B)
should be embedded in the PCIe transaction along with the packet
descriptor, and therefore saves one PCIe transaction between the CPU
and the NIC.
