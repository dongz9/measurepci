\section{Related Work}
\label{sec:related}

\subsection{Interaction between NICs and CPUs}

Previous literatures attmpted to understand the interaction between
NICs and CPUs and its impact on the network latency and
throughput. For example, Larsen et. al~\cite{larsen:2011,larsen:2014}
described the transmit/receive flow of typical Ethernet NICs on modern
Intel platforms and used a PCIe protocol analyzer to quantitively
breakdown the end-to-end network latency between two systems. Their
results demonstrated a substantial portion of latency is introduced by
DMA reads performed at TX NIC, which pull the data from the system
memory to the TX buffer on the TX NIC. Moreover, they showed that when
transmitting small packets (64B in their experiment), an aggregated
PCIe bandwidth of up to 43\% were spent on non-payload traffic. These
somewhat surprising findings suggested the huge potential of
performance improvement. They therefore proposed a solution called
integrated DMA (iDMA), which essentially moves the DMA engine from the
PCIe devices to CPU to eliminate PICe transactions involved with TX/RX
doorbells and packet descriptors.

Our work uses CPU performance counters, instead of a separate PCIe
protocol analyzer, to infer the interactions with InfiniBand NICs and
CPUs. We focus on the behavior of InfiniBand when
transmitting/receiving a batch of packets instead of a single packet.

\subsection{Better NIC Design}

