1. Enabling lcores or ports:
   =========================

	Lcores are enabled through DPDK command line arguments. Ports are enabled
	through variables XIA_R2_PORT_MASK (for the server) and XIA_R0_PORT_MASK
	for the client.

2. Number of queues on a port and mapping between queues and lcores:
   =================================================================

	All queues on a port should be accessed. RSS will push packets to unused
	queues causing RX descriptor exhaustion. Why does this happen when we're
	not issuing rx_burst() for those queues?
	
	At the server:
	==============
		Each lcore should access all ports on its socket. This is not how it
		is done right now. Currently, each lcore accesses all ports regardless
		of the port's socket. This needs to be fixed ASAP.
		
		Assuming that each lcore only accesses all ports on its socket, the
		number of queues on a port is equal to the number of active lcores on
		its socket: this is being done right now.

	At a client:
	============
		Lcore-to-port mapping is fixed and is defined as lcore_to_port[] in 
		client.c. The following constraints hold:
			1. Each lcore only accesses one port.
			2. Each port is accessed by 3 lcores.

		So, each port is initialized with 3 queues. The code knows which lcores
		will use these queues using the client_port_queue_to_lcore() function.
		It is required that the lcore returned by this function be enabled.

	For xia-router0/1 (Westmere machines), ports connect to the I/O hub and
	don't have a socket. However, we assume that xge0/1 are connected to
	socket 0 and xge2/3 are connected to socket 1, and we use a special function
	to find the socket: get_socket_id_from_macaddr().

3. Performance comments:
   =====================

    * The code uses socket #1 on all machines. This is because the NIC for xge2,3 
	on xia-router2 uses a PCIe 2.0 x4 link.
		* TODO: Is this still true?
    
    * xia-router0 can only generate 13.8 Mpps each on xge2 and xge3. This is OK because
    all this load goes to xge4 and xge5 on xia-router2 which can only handle a total of
    24 Mpps (~11.9 Mpps each on xge4 and xge5).
    
    * xia-router0/1 can generate 14.88 Mpps on both ports of a NIC. This is strange
    because we believed that the per-NIC limit was 24 Mpps. Possible reasons:
        1. The 24 Mpps per-NIC limit is for RX.
        2. xia-router0/1 are Westmere machines with an I/O hub.

	* Latency:
		Minimum average RTT between xia-router0/1 and xia-router2 is around 16 us.

	* Price of buffered TX:
		Buffered TX reduces the echo performance of an lcore. Without buffered TX,
		one lcore can echo 17.6 Mpps, but with buffered TX, only 14.4 Mpps.
		
		However, non-buffered TX can only be done if packets are being returned
		on the port they were received on.

4. Packet formatting for IPv4 packets:
   ===================================

	The Ethernet and IPv4 header take the 1st 34 bytes of a packet. Assume 36
	for alignment. This leaves us 24 bytes for data.

	36-39:	lcore_id of the client that sent this packet
	40-47:	client's tsc for end-to-end latency measurement
	48-55:	server's tsc for server-only latency measurement	

5. IPv6 forwarding:
   ================

	Based on DPDK's l3fwd's simple_ipv6_fwd_4pkts

	Formatting:
	===========

		The Ethernet (14 bytes) and IPv6 header (40 bytes) take the 1st 54 bytes
		of a packet. Assume 56 for alignment.

		56-63:	Client's TSC (this comes first because 56 is 8-byte aligned)
		64-68:  Client's lcore_id.

	RSS issue:
	==========
		Couldn't get IPv6 RSS to work, so IPv6 packets are disguised as IPv4 in the
		Ethernet header causing IPv4 RSS to run. IPv4 RSS runs fine on IPv6 packets
		because the src/dst addrs interpreted as IPv4 fall in the src addr of the
		IPv6 packet.

		An important detail is that the IPv4 version_ihl field needs to be set for
		IPv4 RSS. This messes up the vtc_flow field of the corresponding IPv6 packet,
		but that doesn't affect us.


6. Running on xia-router* systems:
   ===============================

	* Reboot the machines
	* At xia-router`i`, run systemish/scripts/dpdk/xia-router`i`-init.sh
	* Mount the fastpp directory on xia-router1 and xia-router0 via sshfs
	* At xia-router2: ./run-servers.sh
	* At xia-router1 and xia-router2: ./run-client.sh [0,1]
